from lxml import html
import get_proxy
import urllib.request
import urllib.parse
import time
import re
import data
import random

#----------------Google search query info------------------#
# spaces are converted to +
# query is given by ?q=query
# page is given by the index of the first search result,
# i.e. &start=20
# sample query: http://www.google.se/search?q=cp+nice&start=20
#----------------------------------------------------------#

sys_debug = True

# Strips the url gathered by the scraper.
# This is necessary since each url is a request
# to google.
def stripUrl(url) :
    regex = '(?=&sa=).+'
    txt = ''.join(url)
    txt = txt.replace("['/url?q=", "")
    txt = re.sub(regex, '', txt)
    if (sys_debug) :
        print(txt)
    return txt

# Attempts to get a specific page from google.
# If that page is not received, the program will keep
# retrying until the user interrupts the program.
# If useproxy then the program will try to get a new
# proxy every iteration.
def getGoogleSearch(query, page, headers, useproxy) :
    start = str(page*10)
    search = "https://www.google.com/search?q="+query+"&start="+start
    if (sys_debug) :
        print("[*]Running google search:\n" + search)
    req = urllib.request.Request(search, None, headers)
    while (True) :
        try :
            response = urllib.request.urlopen(req)
            break
        except Exception as e:
            print(e)
            time.sleep(10)
            if (useproxy) :
                print("[*]Updating proxy!")
                updateProxy()
    src = response.read()
    return str(src)

# Takes the source code of a google search.
# The argument i dedicates exactly what link the function
# will get. The function then strips that link and returns it.
def getURL(tree, i) :
    if (sys_debug) :
        print("[*]GooglePage element: "+str(tree))
    pathOl = "//table[@id='mn']/tbody[@id='desktop-search']/tr/td[2]/div[1]/div[2]/div[2]/div/ol"
    pathA = "/li[%d]/h3/a/@href" % i
    
    div = tree.xpath(pathOl + pathA)
    lnk = str(div)
    url = urllib.parse.unquote(lnk) # removed decode
    if (sys_debug) :
        if (lnk == "[]") :
            print("[-]Empty link element: " + lnk)
        else :
            print("[+]Found link:" + url)
    url = stripUrl(url)
    return url

# Gets every url from a google search query.
def getLinks(query, headers, pages, useproxy) :
    page = 0
    googleLinks = []
    while (page <= pages) :
        time.sleep(random.randrange(10))
        src = getGoogleSearch(query, page, headers, useproxy)
        tree = html.fromstring(src)
        linknr = 1    
        while (True) :
            lnk = getURL(tree, linknr)
            if (lnk != "[]") :
                googleLinks.append(lnk)
            else :
                # If link elements turn up empty, go to next google page
                break
            linknr += 1
            
        page += 1
    return googleLinks
    
# Updates the proxy, utilizes a blacklist to ensure
# that the function does not try the same proxy over and over
def updateProxy() :
    proxyBlacklist = []
    while (True) :
        prox = get_proxy.getHttpProxy(1, False, True, proxyBlacklist)
        if (prox == None or len(prox) == 0) :
            time.sleep(5)
            continue
        proxf = get_proxy.formatP(prox)[0]
        proxy = urllib.request.ProxyHandler ({
            'http':proxf
        })
        opener = urllib.request.build_opener(proxy)
        urllib.request.install_opener(opener)
        if (get_proxy.validateProxy() != get_proxy.TIMEOUT) :
            break
        if (sys_debug) :
            print("Adding proxy " + str(prox[0][0]) + " to blacklist!")
        proxyBlacklist.append(prox[0][0])

# The main function, which treats the query and
# makes the necessary function calls.
def main(query, pages, useproxy=False) :
    #Handle query
    query  = query.replace(" ", "+")
    #Set initial proxy value
    if (useproxy) :
        updateProxy()
    print("[*]Gathering google links!")
    googleLinks = getLinks(query, data.headers, pages, useproxy)
    if (sys_debug) :
        print("[+]Links found:\n" + str(googleLinks))
    return googleLinks        
    
if __name__=='__main__' :
    main("Hello World!",1, False)
