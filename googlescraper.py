from lxml import html
import getproxy
import urllib.request
import urllib.parse
import time
import re
import random

#----------------Google search query info------------------#
# spaces are converted to +
# query is given by ?q=query
# page is given by the index of the first search result,
# i.e. &start=20
# sample query: http://www.google.se/search?q=cp+nice&start=20
#----------------------------------------------------------#

userAgent = 'Mozilla/5.0'
sys_debug = True

def stripUrl(url) :
    regex = '(?=&sa=).+'
    txt = ''.join(url)
    txt = txt.replace("['/url?q=", "")
    txt = re.sub(regex, '', txt)
    if (sys_debug) :
        print(txt)
    return txt

def getGoogleSearch(query, page, headers, useproxy) :
    start = str(page*10)
    search = "https://www.google.com/search?q="+query+"&start="+start
    if (sys_debug) :
        print("[*]Running google search:\n" + search)
        print("[*]UserAgent: " + userAgent)
    req = urllib.request.Request(search, None, headers)
    while (True) :
        try :
            response = urllib.request.urlopen(req)
            break
        except Exception as e:
            print(e)
            time.sleep(10)
            if (useproxy) :
                print("[*]Updating proxy!")
                updateProxy()
    src = response.read()
    return str(src)

def analyzeSource(tree, i) :
    if (sys_debug) :
        print("[*]GooglePage element: "+str(tree))
    pathOl = "//table[@id='mn']/tbody[@id='desktop-search']/tr/td[2]/div[1]/div[2]/div[2]/div/ol"
    pathA = "/li[%d]/h3/a/@href" % i
    
    div = tree.xpath(pathOl + pathA)
    lnk = str(div)
    url = urllib.parse.unquote(lnk) # removed decode
    if (sys_debug) :
        if (lnk == "[]") :
            print("[-]Empty link element: " + lnk)
        else :
            print("[+]Found link:" + url)
    url = stripUrl(url)
    return url

def getLinks(query, headers, pages, useproxy) :
    page = 0
    googleLinks = []
    while (page <= pages) :
        time.sleep(random.randrange(10))
        #if (useproxy) :
        #    updateProxy()
        src = getGoogleSearch(query, page, headers, useproxy)
        tree = html.fromstring(src)
        linknr = 1    
        while (True) :
            lnk = analyzeSource(tree, linknr)
            if (lnk != "[]") :
                googleLinks.append(lnk)
            else :
                # If link elements turn up empty, go to next google page
                break
            linknr += 1
            
        page += 1
    return googleLinks
    
def updateProxy() :
    proxyBlacklist = []
    while (True) :
        prox = getproxy.getHttpProxy(1, False, True, proxyBlacklist)
        if (prox == None or len(prox) == 0) :
            time.sleep(5)
            continue
        proxf = getproxy.formatP(prox)[0]
        proxy = urllib.request.ProxyHandler ({
            'http':proxf
        })
        opener = urllib.request.build_opener(proxy)
        urllib.request.install_opener(opener)
        if (getproxy.validateProxy() != getproxy.TIMEOUT) :
            break
        if (sys_debug) :
            print("Adding proxy " + str(prox[0][0]) + " to blacklist!")
        proxyBlacklist.append(prox[0][0])


def main(query, pages, useproxy=False) :
    #Handle query
    query  = query.replace(" ", "+")
    headers = { 'User-Agent' : userAgent }
    print("[*]Gathering google links!")
    googleLinks = getLinks(query, headers, pages, useproxy)
    if (sys_debug) :
        print("[+]Links found:\n" + str(googleLinks))
    return googleLinks        
    
if __name__=='__main__' :
    main("Hello World!",5, True)
